{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data:\n",
      "      Id                                             Prompt  \\\n",
      "0  11527  [INST] You are an AI assistant that helps peop...   \n",
      "1   7322  [INST] You are an AI assistant. You will be gi...   \n",
      "2  11742  [INST] You are an AI assistant. You will be gi...   \n",
      "3  20928  [INST] You are an AI assistant. User will you ...   \n",
      "4  25830  [INST] You are an AI assistant. User will you ...   \n",
      "\n",
      "                                              Answer  Target  \n",
      "0  Step-by-step reasoning process:\\n1. Randy spen...       0  \n",
      "1  What is the temperature at which hypothermia b...       0  \n",
      "2  Answer: c) No. \\n\\nThe hypothesis is false bec...       0  \n",
      "3                                         Prismatoid       0  \n",
      "4                                             Case B       0  \n",
      "\n",
      "Test Data:\n",
      "      Id                                             Prompt  \\\n",
      "0  20568  [INST] You are an AI assistant. You will be gi...   \n",
      "1  17686  question:Question: This article: According to ...   \n",
      "2  13035  [INST] You are an AI assistant. Provide a deta...   \n",
      "3  22646  [INST] You are an AI assistant. User will you ...   \n",
      "4   5535  [INST] You are an AI assistant. You will be gi...   \n",
      "\n",
      "                                              Answer  \n",
      "0  London Irish lost to Grenoble 21-6 in the Euro...  \n",
      "1                                              10.2%  \n",
      "2                                       Can't answer  \n",
      "3  'Ahora, por lo tanto, no eres tú quien me enví...  \n",
      "4  c). can decrease blood pressure \\n d). can dec...  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load training data\n",
    "train_data = pd.read_csv('train.csv')  # replace 'train.csv' with your actual file path\n",
    "prompts_train = train_data['Prompt'].tolist()\n",
    "answers_train = train_data['Answer'].tolist()\n",
    "labels_train = train_data['Target'].tolist()  # Target is present in train.csv\n",
    "\n",
    "# Load test data (this does not have 'Target')\n",
    "test_data = pd.read_csv('test.csv')  # replace 'test.csv' with your actual file path\n",
    "prompts_test = test_data['Prompt'].tolist()\n",
    "answers_test = test_data['Answer'].tolist()\n",
    "\n",
    "# Check the first few rows of both datasets\n",
    "print(\"Training Data:\")\n",
    "print(train_data.head())\n",
    "print(\"\\nTest Data:\")\n",
    "print(test_data.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yasas\\AppData\\Roaming\\Python\\Python310\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-11): 12 x BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "\n",
    "# Load BERT tokenizer and model\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Set the model to evaluation mode (disable gradient calculation)\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating BERT embeddings for prompts...\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import numpy as np\n",
    "\n",
    "# Load data (train.csv)\n",
    "data = pd.read_csv('train.csv')\n",
    "\n",
    "# Extract prompts and answers\n",
    "prompts_train = data['Prompt'].tolist()\n",
    "answers_train = data['Answer'].tolist()\n",
    "\n",
    "# Check for rows where either the prompt or answer is missing or empty\n",
    "filtered_prompts, filtered_answers = [], []\n",
    "for prompt, answer in zip(prompts_train, answers_train):\n",
    "    if isinstance(prompt, str) and isinstance(answer, str) and prompt.strip() and answer.strip():\n",
    "        filtered_prompts.append(prompt)\n",
    "        filtered_answers.append(answer)\n",
    "\n",
    "# Load the BERT model and tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Function to get BERT embeddings with batch processing\n",
    "def get_bert_embeddings(text_list, batch_size=32):\n",
    "    embeddings = []\n",
    "    for i in range(0, len(text_list), batch_size):\n",
    "        batch_text = text_list[i:i+batch_size]\n",
    "        \n",
    "        # Tokenize the batch and convert to PyTorch tensors\n",
    "        inputs = tokenizer(batch_text, return_tensors='pt', padding=True, truncation=True, max_length=128)\n",
    "        \n",
    "        # Forward pass through BERT to get embeddings\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "            # Take the mean of the last hidden states to get the sentence embedding\n",
    "            batch_embeddings = outputs.last_hidden_state.mean(dim=1).numpy()\n",
    "            embeddings.append(batch_embeddings)\n",
    "    \n",
    "    # Concatenate all batches to form the final embedding matrix\n",
    "    return np.concatenate(embeddings)\n",
    "\n",
    "# Generate embeddings for filtered training data\n",
    "print(\"Generating BERT embeddings for prompts...\")\n",
    "prompt_embeddings_train = get_bert_embeddings(filtered_prompts)\n",
    "\n",
    "print(\"Generating BERT embeddings for answers...\")\n",
    "answer_embeddings_train = get_bert_embeddings(filtered_answers)\n",
    "\n",
    "# Check the shape of the embeddings\n",
    "print(\"Filtered prompt embeddings shape:\", prompt_embeddings_train.shape)\n",
    "print(\"Filtered answer embeddings shape:\", answer_embeddings_train.shape)\n",
    "\n",
    "# Concatenate prompt and answer embeddings for training data\n",
    "combined_embeddings_train = np.concatenate([prompt_embeddings_train, answer_embeddings_train], axis=1)\n",
    "\n",
    "# Check final embeddings shape\n",
    "print(\"Combined embeddings shape:\", combined_embeddings_train.shape)\n",
    "\n",
    "# Save the embeddings for later use if needed\n",
    "np.save('combined_embeddings_train.npy', combined_embeddings_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered Labels shape: (16668,)\n",
      "Training data shape: (13334, 1536)\n",
      "Validation data shape: (3334, 1536)\n",
      "Training labels shape: (13334,)\n",
      "Validation labels shape: (3334,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "# Get labels (Targets) from your dataset\n",
    "labels_train = data['Target'].tolist()\n",
    "\n",
    "# Filter labels to match filtered prompts and answers\n",
    "filtered_labels = []\n",
    "for i, (prompt, answer) in enumerate(zip(prompts_train, answers_train)):\n",
    "    if isinstance(prompt, str) and isinstance(answer, str) and prompt.strip() and answer.strip():\n",
    "        filtered_labels.append(labels_train[i])\n",
    "\n",
    "# Convert the filtered labels to a NumPy array\n",
    "labels_train_np = np.array(filtered_labels)\n",
    "print(\"Filtered Labels shape:\", labels_train_np.shape)\n",
    "\n",
    "# Split the combined embeddings and labels into training and validation sets (80% train, 20% validation)\n",
    "X_train, X_val, y_train, y_val = train_test_split(combined_embeddings_train, labels_train_np, test_size=0.2, random_state=42)\n",
    "\n",
    "# Print shapes to confirm\n",
    "print(\"Training data shape:\", X_train.shape)\n",
    "print(\"Validation data shape:\", X_val.shape)\n",
    "print(\"Training labels shape:\", y_train.shape)\n",
    "print(\"Validation labels shape:\", y_val.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the Logistic Regression classifier...\n",
      "Validation Accuracy: 94.51%\n",
      "Classification Report:\n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "No Hallucination       0.96      0.98      0.97      3176\n",
      "   Hallucination       0.33      0.16      0.21       158\n",
      "\n",
      "        accuracy                           0.95      3334\n",
      "       macro avg       0.65      0.57      0.59      3334\n",
      "    weighted avg       0.93      0.95      0.94      3334\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Initialize the Logistic Regression model\n",
    "clf = LogisticRegression(max_iter=1000)  # Increase max_iter to ensure convergence\n",
    "\n",
    "# Train the classifier on the training set\n",
    "print(\"Training the Logistic Regression classifier...\")\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the validation set\n",
    "y_pred = clf.predict(X_val)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_val, y_pred)\n",
    "print(f\"Validation Accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "# Get a detailed classification report (precision, recall, F1-score)\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_val, y_pred, target_names=['No Hallucination', 'Hallucination']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to logistic_regression_model.pkl\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "# Save the trained Logistic Regression model to a file\n",
    "model_filename = 'logistic_regression_model.pkl'\n",
    "joblib.dump(clf, model_filename)\n",
    "\n",
    "print(f\"Model saved to {model_filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating BERT embeddings for filtered test prompts...\n",
      "Generating BERT embeddings for filtered test answers...\n",
      "Test Predictions: [0 1 0 ... 0 0 0]\n",
      "Predictions saved to test_predictions.csv\n"
     ]
    }
   ],
   "source": [
    "# Ensure the test data is properly cleaned by filtering out invalid rows\n",
    "filtered_prompts_test, filtered_answers_test = [], []\n",
    "for prompt, answer in zip(prompts_test, answers_test):\n",
    "    if isinstance(prompt, str) and isinstance(answer, str) and prompt.strip() and answer.strip():\n",
    "        filtered_prompts_test.append(prompt)\n",
    "        filtered_answers_test.append(answer)\n",
    "\n",
    "# Generate BERT embeddings for the filtered test data (prompts and answers)\n",
    "print(\"Generating BERT embeddings for filtered test prompts...\")\n",
    "prompt_embeddings_test = get_bert_embeddings(filtered_prompts_test)\n",
    "\n",
    "print(\"Generating BERT embeddings for filtered test answers...\")\n",
    "answer_embeddings_test = get_bert_embeddings(filtered_answers_test)\n",
    "\n",
    "# Concatenate prompt and answer embeddings for the test set\n",
    "combined_embeddings_test = np.concatenate([prompt_embeddings_test, answer_embeddings_test], axis=1)\n",
    "\n",
    "# Load the trained model\n",
    "clf = joblib.load('logistic_regression_model.pkl')\n",
    "\n",
    "# Make predictions on the test data\n",
    "test_predictions = clf.predict(combined_embeddings_test)\n",
    "\n",
    "# Output the predictions (1: Hallucination, 0: No Hallucination)\n",
    "print(\"Test Predictions:\", test_predictions)\n",
    "\n",
    "# Save predictions to a CSV file if needed\n",
    "output_df = pd.DataFrame({'Id': test_data['Id'][:len(test_predictions)], 'Prediction': test_predictions})\n",
    "output_df.to_csv('test_predictions.csv', index=False)\n",
    "\n",
    "print(\"Predictions saved to test_predictions.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'LabelEncoder' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 13\u001b[0m\n\u001b[0;32m     10\u001b[0m         filtered_labels\u001b[38;5;241m.\u001b[39mappend(label)\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# Convert the labels to numeric encoding\u001b[39;00m\n\u001b[1;32m---> 13\u001b[0m label_encoder \u001b[38;5;241m=\u001b[39m \u001b[43mLabelEncoder\u001b[49m()\n\u001b[0;32m     14\u001b[0m y_encoded \u001b[38;5;241m=\u001b[39m label_encoder\u001b[38;5;241m.\u001b[39mfit_transform(filtered_labels)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# Now that the filtering is consistent, you can split the data\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'LabelEncoder' is not defined"
     ]
    }
   ],
   "source": [
    "# Assuming data['Target'] contains the target labels\n",
    "y_train = data['Target'].tolist()\n",
    "\n",
    "# Filter the prompts, answers, and target labels simultaneously\n",
    "filtered_prompts, filtered_answers, filtered_labels = [], [], []\n",
    "for prompt, answer, label in zip(prompts_train, answers_train, y_train):\n",
    "    if isinstance(prompt, str) and isinstance(answer, str) and prompt.strip() and answer.strip():\n",
    "        filtered_prompts.append(prompt)\n",
    "        filtered_answers.append(answer)\n",
    "        filtered_labels.append(label)\n",
    "\n",
    "# Convert the labels to numeric encoding\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(filtered_labels)\n",
    "\n",
    "# Now that the filtering is consistent, you can split the data\n",
    "X = np.concatenate([prompt_embeddings_train, answer_embeddings_train], axis=1)  # Make sure to use filtered embeddings\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y_encoded, test_size=0.2, random_state=42, stratify=y_encoded)\n",
    "\n",
    "# Proceed with evaluating classifiers as before\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Logistic Regression...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [13334, 16687]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 67\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m clf_name, clf \u001b[38;5;129;01min\u001b[39;00m classifiers\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m     66\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEvaluating \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mclf_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 67\u001b[0m     \u001b[43mevaluate_classifier\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclf_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     69\u001b[0m \u001b[38;5;66;03m# Display the results\u001b[39;00m\n\u001b[0;32m     70\u001b[0m \u001b[38;5;28mprint\u001b[39m(results_df)\n",
      "Cell \u001b[1;32mIn[11], line 42\u001b[0m, in \u001b[0;36mevaluate_classifier\u001b[1;34m(clf_name, clf, X_train, y_train, X_val, y_val)\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mevaluate_classifier\u001b[39m(clf_name, clf, X_train, y_train, X_val, y_val):\n\u001b[0;32m     41\u001b[0m     \u001b[38;5;66;03m# Train the classifier\u001b[39;00m\n\u001b[1;32m---> 42\u001b[0m     \u001b[43mclf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     44\u001b[0m     \u001b[38;5;66;03m# Predictions\u001b[39;00m\n\u001b[0;32m     45\u001b[0m     y_train_pred \u001b[38;5;241m=\u001b[39m clf\u001b[38;5;241m.\u001b[39mpredict(X_train)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\base.py:1474\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1467\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1469\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1470\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1471\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1472\u001b[0m     )\n\u001b[0;32m   1473\u001b[0m ):\n\u001b[1;32m-> 1474\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\linear_model\\_logistic.py:1201\u001b[0m, in \u001b[0;36mLogisticRegression.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m   1198\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1199\u001b[0m     _dtype \u001b[38;5;241m=\u001b[39m [np\u001b[38;5;241m.\u001b[39mfloat64, np\u001b[38;5;241m.\u001b[39mfloat32]\n\u001b[1;32m-> 1201\u001b[0m X, y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1202\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1203\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1204\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1205\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1206\u001b[0m \u001b[43m    \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mC\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1207\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_large_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msolver\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mliblinear\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msag\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msaga\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1208\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1209\u001b[0m check_classification_targets(y)\n\u001b[0;32m   1210\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses_ \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39munique(y)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\base.py:650\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[0;32m    648\u001b[0m         y \u001b[38;5;241m=\u001b[39m check_array(y, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_y_params)\n\u001b[0;32m    649\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 650\u001b[0m         X, y \u001b[38;5;241m=\u001b[39m check_X_y(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params)\n\u001b[0;32m    651\u001b[0m     out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[0;32m    653\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m check_params\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mensure_2d\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m):\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\utils\\validation.py:1281\u001b[0m, in \u001b[0;36mcheck_X_y\u001b[1;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[0;32m   1263\u001b[0m X \u001b[38;5;241m=\u001b[39m check_array(\n\u001b[0;32m   1264\u001b[0m     X,\n\u001b[0;32m   1265\u001b[0m     accept_sparse\u001b[38;5;241m=\u001b[39maccept_sparse,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1276\u001b[0m     input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1277\u001b[0m )\n\u001b[0;32m   1279\u001b[0m y \u001b[38;5;241m=\u001b[39m _check_y(y, multi_output\u001b[38;5;241m=\u001b[39mmulti_output, y_numeric\u001b[38;5;241m=\u001b[39my_numeric, estimator\u001b[38;5;241m=\u001b[39mestimator)\n\u001b[1;32m-> 1281\u001b[0m \u001b[43mcheck_consistent_length\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1283\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m X, y\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\utils\\validation.py:457\u001b[0m, in \u001b[0;36mcheck_consistent_length\u001b[1;34m(*arrays)\u001b[0m\n\u001b[0;32m    455\u001b[0m uniques \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39munique(lengths)\n\u001b[0;32m    456\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(uniques) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m--> 457\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    458\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound input variables with inconsistent numbers of samples: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    459\u001b[0m         \u001b[38;5;241m%\u001b[39m [\u001b[38;5;28mint\u001b[39m(l) \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m lengths]\n\u001b[0;32m    460\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [13334, 16687]"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.ensemble import ExtraTreesClassifier, BaggingClassifier\n",
    "from sklearn.linear_model import RidgeClassifier, Perceptron\n",
    "\n",
    "# List of classifiers\n",
    "classifiers = {\n",
    "    \"Logistic Regression\": LogisticRegression(max_iter=1000),\n",
    "    \"Random Forest\": RandomForestClassifier(),\n",
    "    \"SVM\": SVC(),\n",
    "    \"KNN\": KNeighborsClassifier(),\n",
    "    \"Decision Tree\": DecisionTreeClassifier(),\n",
    "    \"Naive Bayes\": GaussianNB(),\n",
    "    \"Gradient Boosting\": GradientBoostingClassifier(),\n",
    "    \"AdaBoost\": AdaBoostClassifier(),\n",
    "    \"MLP Classifier\": MLPClassifier(max_iter=1000),\n",
    "    \"LDA\": LinearDiscriminantAnalysis(),\n",
    "    \"QDA\": QuadraticDiscriminantAnalysis(),\n",
    "    \"Extra Trees\": ExtraTreesClassifier(),\n",
    "    \"Bagging Classifier\": BaggingClassifier(),\n",
    "    \"Ridge Classifier\": RidgeClassifier(),\n",
    "    \"Perceptron\": Perceptron(),\n",
    "}\n",
    "\n",
    "# Initialize a DataFrame to store the results\n",
    "results_df = pd.DataFrame(columns=[\n",
    "    \"Classifier\", \"Train Accuracy\", \"Val Accuracy\", \"Train F1\", \"Val F1\", \n",
    "    \"Train Precision\", \"Val Precision\", \"Train Recall\", \"Val Recall\"\n",
    "])\n",
    "\n",
    "# Function to evaluate and store metrics\n",
    "def evaluate_classifier(clf_name, clf, X_train, y_train, X_val, y_val):\n",
    "    # Train the classifier\n",
    "    clf.fit(X_train, y_train)\n",
    "    \n",
    "    # Predictions\n",
    "    y_train_pred = clf.predict(X_train)\n",
    "    y_val_pred = clf.predict(X_val)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    train_acc = accuracy_score(y_train, y_train_pred)\n",
    "    val_acc = accuracy_score(y_val, y_val_pred)\n",
    "    train_f1 = f1_score(y_train, y_train_pred, average='weighted')\n",
    "    val_f1 = f1_score(y_val, y_val_pred, average='weighted')\n",
    "    train_precision = precision_score(y_train, y_train_pred, average='weighted')\n",
    "    val_precision = precision_score(y_val, y_val_pred, average='weighted')\n",
    "    train_recall = recall_score(y_train, y_train_pred, average='weighted')\n",
    "    val_recall = recall_score(y_val, y_val_pred, average='weighted')\n",
    "    \n",
    "    # Append the results to the DataFrame\n",
    "    results_df.loc[len(results_df)] = [\n",
    "        clf_name, train_acc, val_acc, train_f1, val_f1, \n",
    "        train_precision, val_precision, train_recall, val_recall\n",
    "    ]\n",
    "\n",
    "# Loop through classifiers and evaluate each one\n",
    "for clf_name, clf in classifiers.items():\n",
    "    print(f\"Evaluating {clf_name}...\")\n",
    "    evaluate_classifier(clf_name, clf, X_train, y_train, X_val, y_val)\n",
    "\n",
    "# Display the results\n",
    "print(results_df)\n",
    "\n",
    "# Optionally, save the results to a CSV file\n",
    "results_df.to_csv('classifier_comparison_results.csv', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
